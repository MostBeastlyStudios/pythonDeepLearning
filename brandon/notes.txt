

Saving a Neural Network:
  > In python, you can save specific objects as "Pickles"
  > Do not have to train every time

Linear Regression:
  > Looks at past data and tries to create a best fit line for it. Continues that trend on 2D data. Matches variance and step size.
  > Useful for 2D data
    > XY graph

    Best fit line:
      > Y = mx+b
        > m = (mean(x)*mean(y)-mean(xy))/(mean(x)^2 - mean(x^2))
        > b = mean(y)-m*mean(x)
      > Only works on 2D data

K Nearest Neighbors:
  > Sorting unknown data based on who is the closest in proximity on a graph
  > Looks at all points which can be classified and any unclassifiable data points are classified by the K nearest neighbors.
  > Not great on large data sets because it relies on Euclidean distance calculation
  > No training involved

    Euclidean Distance:
      > sqrt(nEi=1 (ai=Pi)^2)

Support Vector Machine:
  > Best for reading handwritten data
  > Separates data into two groups using a "best separating hyperplane" or "decision line"
  > Can only separate data into two groups at a time for classification
  > Need to know vector math
    > Dot product
  > Needs to be trained
  > If your data is not linearly separable, you need to add dimensions until it is
    > Incredibly expensive, so you can use Kernels to make it less expensive
  > Because it's a binary classifier, you can use OVR (One Vs. Rest) or OVO (One vs One)

Clustering + Unsupervised Machine Learning:
  > Class clustering
      > You tell the machine how many clusters
      > K-Means
  > Hierarchical Clustering
      > Machine groups into a number of clusters
      > Determines how many groups there should be
      > Mean Shift
  > Typically used as a precursor to a SVM or NN

K-Means Clustering
  > You choose the value of K. The number of clusters
  > Plots data and chooses three random centroids
  > Groups data depending on proximity to centroids
  > Find the center of those clusters/groups and make that the new centroid
  > Repeat this until the centroids are no longer moving
      > Max steps for training and accuracy tolerance
  > Takes a long time to train, but then it's really fast

Mean Shift Clustering
  > Each feature is it's own cluster centroid
  > Each feature has a radius around it
  > All of the features within that radius calculate a centroid for a new circle
  > Repeat this with the new centroid with same sized radius
  > All of the new points converge into a single point
  > Each cluster center separates and becomes optimized clusters

Neural Networks:
  > Invented in 1940 but 100% theoretical
  > Not usable until 2011 when computing power caught up
  > Inputs are sent to a neuron to be summed. The connections are weighted. The neuron fires a value dependent on the bias from the activation scaler.
  > Deep Neural Network:
      > Has more than one hidden layer
  > Needs a much larger training set
      > Up to 5 million features
  > Take much longer to train because there is a much larger number of variables to optimize

  > Training
      > '''
      input > weight > hidden layer 1 (activation function) > weights > hidden layer 2
      (activation function) > weights > output layer

      ^^^ Feed forward NN because it goes straight through

      Compare output to intended output > cost function (cross entropy)
      Optimization function (optimizer) > minimize cost (AdamOptimizer, SGD, AdaGrad, etc.)

      ^^^ Back propogation because it resets variables based on intended results

      feed forward + backprop = epoch

      Training is in epoch iterations
      '''

TensorFlow:
  > Everything is handled as "Tensors" or arrays
  > All processing is done through sessions
  > Two main steps
      > Build computation graph (modeling)
      > Build session
